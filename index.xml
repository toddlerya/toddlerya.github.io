<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>日拱一卒</title>
    <link>https://toddlerya.github.io/</link>
    <description>Recent content on 日拱一卒</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 05 Jan 2019 12:08:32 +0800</lastBuildDate>
    
	<atom:link href="https://toddlerya.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Hello Docker</title>
      <link>https://toddlerya.github.io/post/hello-docker/</link>
      <pubDate>Sat, 05 Jan 2019 12:08:32 +0800</pubDate>
      
      <guid>https://toddlerya.github.io/post/hello-docker/</guid>
      <description>说起来Docker，大家都或多或少有耳闻，但是我的公司因为业务场景，长尾效应，至今为止Docker只在小部分项目轻度使用。但是这不妨碍我们主动学习，应用Docker，因此，从2018年开始，我就推动Docker在测试部应用。我们的测试平台，GitLab、JIRA、Confluence等都是通过Docker部署的。
团队内部做算法测试的同学搭建开发测试环境需要安装各种第三方库，因为工作环境是内网，无法连接互联网进行安装，痛苦不堪，使用Docker将相关环境在外网构建好传入内网使用是比较方便的方式；还有工具链建设、测试平台建设用到的各种基础设施MySQL、Nginx等快速部署，使用Docker都是最佳方案。为了方便团队同学了解Docker，团队老大让我来进行一次内部Docker入门分享。
没有Docker的从前 为了提高服务器利用效率，我们将一台高性能的服务器通过vSphere虚拟化，创建多台虚拟机，提供相对独立环境隔离的操作系统环境和计算资源，我们可以对每一台虚拟机灵活分配计算资源、存储资源等，还可以对每一台虚拟机创建多个快照，看起来很棒！是的，在传统的软件开发模式下，这种方式挺好，但是这并没有解决软件的部署效率低，交付困难，开发环境与测试环境不一致，测试环境与生产环境不一致等问题，这也是为什么我们有了持续集成，却没有做好持续交付的一大根本问题。
有了Docker的现在 各大互联网厂商都在使用Docker，通过Docker结合微服务等技术他们将DevOps这一理念落地了，真正做到了持续集成、持续部署、持续交付，快速迭代，可以实现一天多次版本上线发布！回头看有了Docker我们在日常工作中能做什么，举个最简单的例子：
以往我们想部署三套环境（一套Dev环境、一套Test环境、一套Release环境），需要三个MySQL数据库，需要怎么做呢？
 方案1：准备三台服务器，分别安装MySQL，干干净净，互不干扰，但是浪费资源啊； 方案2：一台服务器部署三个MySQL实例，分别配置不同的MySQL数据存储路径、配置文件路径、端口，操作复杂，维护困难，想想都头疼；  用Docker以后呢，我们可以这样做：
docker run --name release-mysql -p 3306:3306 -d mysql:5.7 docker run --name dev-mysql -p 3307:3306 -d mysql:5.7 docker run --name test-mysql -p 3308:3306 -d mysql:5.7  这样我们就有了三个独立的MySQL数据库服务！当然这是个示例，没有做数据卷映射等配置，但那些配置相比于传统方案来说，复杂度不值一提。
Docker到底是什么 Docker 是世界领先的软件容器平台。开发人员利用 Docker 可以消除协作编码时“在我的机器上可正常工作”的问题。运维人员利用 Docker 可以在隔离容器中并行运行和管理应用，获得更好的计算密度。企业利用 Docker 可以构建敏捷的软件交付管道，以更快的速度、更高的安全性和可靠的信誉为 Linux 和 Windows Server 应用发布新功能。
什么是容器呢？  将软件打包成标准化单元，以用于开发、交付和部署  容器镜像是轻量的、可执行的独立软件包，包含软件运行所需的所有内容：代码、运行时环境、系统工具、系统库和设置。容器化软件适用于基于 Linux 和 Windows 的应用，在任何环境中都能够始终如一地运行。容器赋予了软件独立性，使其免受外在环境差异（例如，开发和预演环境的差异）的影响，从而有助于减少团队间在相同基础设施上运行不同软件时的冲突。
容器的优势特点 轻量 在一台机器上运行的多个 Docker 容器可以共享这台机器的操作系统内核；它们能够迅速启动，只需占用很少的计算和内存资源。镜像是通过文件系统层进行构造的，并共享一些公共文件。这样就能尽量降低磁盘用量，并能更快地下载镜像。
标准 Docker 容器基于开放式标准，能够在所有主流 Linux 版本、Microsoft Windows 以及包括 VM、裸机服务器和云在内的任何基础设施上运行。</description>
    </item>
    
    <item>
      <title>无序大数组的中位数算法</title>
      <link>https://toddlerya.github.io/post/%E6%97%A0%E5%BA%8F%E5%A4%A7%E6%95%B0%E7%BB%84%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0%E7%AE%97%E6%B3%95/</link>
      <pubDate>Fri, 25 May 2018 21:29:10 +0000</pubDate>
      
      <guid>https://toddlerya.github.io/post/%E6%97%A0%E5%BA%8F%E5%A4%A7%E6%95%B0%E7%BB%84%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0%E7%AE%97%E6%B3%95/</guid>
      <description>题目要求 最近做了一道题，题目是这样的：
找到一个巨大数组的中位数。 # demo：[1,100,2,5,12,44,88,77,54,932,61]  解题方法 巨大的数组，排序肯定不是最优解了，解题思路可以借鉴快排算法那种分而治之的思想。
详情直接看代码实现吧。
#!/usr/bin/env python # -*- coding:utf-8 -*- # author: toddler import random import statistics import time import sys sys.setrecursionlimit(1000000) def find_mid(mid_index, __list): &amp;quot;&amp;quot;&amp;quot; 寻找中位数算法 :param mid_index: 中位数索引 :param __list: 目标数组 :return: 中位数数值 &amp;quot;&amp;quot;&amp;quot; # 随机取一个数作为分割元素, 以分割元素为界限，将数组分割大小两部分 random_num = random.choice(__list) small_list = [i for i in __list if i &amp;lt; random_num] # 若小数组的右端索引大于中位数索引, 则继续缩小小数组的区间长度, 这样可以直接舍弃比中位数大的元素, 减少计算量 if len(small_list) &amp;gt; mid_index: return find_mid(mid_index, small_list) # 分割点左边的元素没有价值, 被舍弃, 相应的中位数索引左移对应长度, 保证相对原始数据索引长度不变 mid_index -= len(small_list) # 判断分割点有几个, 若分割点所占空间长度大于新的中位数索引, 则分割点就是中位数 same_mid_num = __list.</description>
    </item>
    
    <item>
      <title>微信跳一跳之线性回归算法优化</title>
      <link>https://toddlerya.github.io/post/%E5%BE%AE%E4%BF%A1%E8%B7%B3%E4%B8%80%E8%B7%B3%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96/</link>
      <pubDate>Sun, 07 Jan 2018 12:24:53 +0000</pubDate>
      
      <guid>https://toddlerya.github.io/post/%E5%BE%AE%E4%BF%A1%E8%B7%B3%E4%B8%80%E8%B7%B3%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96/</guid>
      <description>最近微信上线了一款小应用&amp;mdash;“跳一跳”，这个规则简单，让人上瘾的小游戏和2048一样魔性，朋友圈也是各路小伙伴各显神通：硬件流（树莓派+步进电机）、日天流（篡改http请求）、软件流（adb控制手机模拟点击）。
今天我们也来实践下，当然选择最顺手的Python来搞咯，直接找到开源项目wechat_jump_game进行优化改造。此项目有个pull request[优化]跑分17000+ 新增AI模块，机器人自主学习生成跳跃公式，看到AI我们就来了兴趣，只见过理论，还没有实践过，可以拿这个实践下。
这个pull request介绍如下： &amp;gt; 机器人精确采集跳跃结果并自主学习，使用线性回归方法 &amp;gt; 拟合出最优 [按压时间]-&amp;gt;[弹跳距离] 线性公式 Y =kX + b &amp;gt; 本优化无需修改config文件，可以适配所有手机，经过十次以上跳跃学习，机器人即可 &amp;gt; 模拟出相对稳定的线性公式。随着采集结果越多，跳跃也越精确，后期基本连续命中靶心。 &amp;gt; 理论上只要目标点获取无误，会一直跳下去。
工作两年多，一直在做服务端后台应用相关的测试，没接触过移动端测试呢，正好趁这次机会学习下怎么通过代码自动化控制安卓手机。 下面来动手试一下，找出下岗多年的MX3，充电开机。
第一次调试  安装好adb，配置好环境变量。 手机打开开发者模式，连接PC。 命令行测试是否连接成功: adb devices，手机弹出是否信任窗口，点击确定，已经链接成功。 测试一些adb命令是否正常: adb shell wm size，返回信息：Physical size: 1080x1800，完美。 通过virtualenv建立虚拟环境，安装项目所需的第三方库。 手机微信打开跳一跳，点击开始游戏。 运行wechat_jump_auto_ai.py，报错T_T&amp;hellip; 查看代码发现是截图部分操作不适配MX3，手动修改代码后成功截图运行 将 Python screenshot = screenshot.replace(b&#39;\r\n&#39;, b&#39;\n&#39;)  修改为 Python screenshot = screenshot.replace(b&#39;\r\r\n&#39;, b&#39;\n&#39;)   第二次调试 按照程序逻辑，运行十次之后即可采用线性回归算法学习得到的公式，根据已知距离得出按压时间，但实际结果却和一个弱智一样，2分就挂掉了&amp;hellip; 查看代码发现有个魔法数字要自己设置，程序根据这个数字进行截图计算误差:time.sleep(0.2)。
# 在跳跃落下的瞬间 摄像机移动前截图 这个参数要自己校调 time.sleep(0.2) pull_screenshot_temp() im_temp = Image.open(&#39;./autojump_temp.png&#39;) temp_piece_x, temp_piece_y = find_piece(im_temp) debug.</description>
    </item>
    
    <item>
      <title>Golang学习（二）</title>
      <link>https://toddlerya.github.io/post/golang%E5%AD%A6%E4%B9%A0%E4%BA%8C/</link>
      <pubDate>Sun, 31 Dec 2017 16:13:51 +0000</pubDate>
      
      <guid>https://toddlerya.github.io/post/golang%E5%AD%A6%E4%B9%A0%E4%BA%8C/</guid>
      <description>闲扯 2017年的最后一天啦！今天是最后一批90后（1999年12月31日出生）的18岁生日，祝他们生日快乐！明天就是2018年啦，意味着90后已经全部成年，逐步成为社会的中流砥柱啦！ 顺便吐槽下，被朋友圈的18岁照片刷屏啦，岁月是把杀猪刀，18岁的少年们都去哪啦！小伙伴们不要只顾着工作，也注意下身体啊，要变强不要变秃啊！不要变成油腻的中年胖子啊！
祝自己在2018年能更加努力，锻炼好身体！还有很多很多事情等我去做！Fight！
言归正传，在Golang学习（一）那篇博客中我们写了一个小工具，当时还没学习到并发，正好元旦假期学习了下并发，来实践改进下我们的工具。
正文 遗留Bug修复 文件复制过程中异常报错，程序没有退出的问题已经修复，创建文件和打开文件时加入了panic
srcFile, err := os.Open(src) if err != nil { fmt.Println(err) panic(&amp;quot;打开文件错误!&amp;quot;) } defer srcFile.Close() desFile, err := os.Create(des) if err != nil { fmt.Println(err) panic(&amp;quot;创建文件错误!&amp;quot;) } defer desFile.Close()  使用Golang的gorutine来并发，提高性能 创建一个容量与期望生成文件个数大小相当的布尔型的chan，然后循环执行任务，每次向chan写入一个值，并通过读取chan来阻塞main函数的结束，伪代码如下
func main() { c := make(chan bool, *generateFileNumber) for count := 0; count &amp;lt; *generateFileNumber; count++ { dosomething(c) } &amp;lt;-c } func dosomething(c) { do c &amp;lt;- true }  代码 /* * User: toddlerya * Date: 2017/12/23 * Update: 2017/12/31 * ds接入模块加压工具 */ package main import ( &amp;quot;flag&amp;quot; &amp;quot;fmt&amp;quot; &amp;quot;io&amp;quot; &amp;quot;math/rand&amp;quot; &amp;quot;os&amp;quot; &amp;quot;runtime&amp;quot; &amp;quot;strconv&amp;quot; &amp;quot;strings&amp;quot; &amp;quot;time&amp;quot; ) func judgeExists(name string) bool { if _, err := os.</description>
    </item>
    
    <item>
      <title>开源工具集--备忘清单</title>
      <link>https://toddlerya.github.io/post/%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E9%9B%86-%E5%A4%87%E5%BF%98%E6%B8%85%E5%8D%95/</link>
      <pubDate>Sat, 30 Dec 2017 21:37:25 +0000</pubDate>
      
      <guid>https://toddlerya.github.io/post/%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7%E9%9B%86-%E5%A4%87%E5%BF%98%E6%B8%85%E5%8D%95/</guid>
      <description>经常看到一些有趣的开源工具，没机会去尝试，时间久了就忘记还有这些轮子啦，开个坑做个备忘录，记录下。
监控系统  open-falcon Prometheus Cockpit  数据分析/BI  Metabase  数据库  时间序列数据库influxdb  2017年10月22日 于 南京
Email
GitHub</description>
    </item>
    
    <item>
      <title>探究inode耗尽导致的no space left on device报错原因</title>
      <link>https://toddlerya.github.io/post/inode%E8%80%97%E5%B0%BD%E5%AF%BC%E8%87%B4%E7%9A%84no-space-left-on-device%E6%8A%A5%E9%94%99/</link>
      <pubDate>Sun, 24 Dec 2017 13:57:10 +0000</pubDate>
      
      <guid>https://toddlerya.github.io/post/inode%E8%80%97%E5%B0%BD%E5%AF%BC%E8%87%B4%E7%9A%84no-space-left-on-device%E6%8A%A5%E9%94%99/</guid>
      <description>在Golang学习（一）那篇博客中，我们提到了由于inode被耗尽导致的磁盘存储空间不足的报错，现在我们就来深入了解下inode。
什么是inode？ 理解inode，要从文件储存说起。 文件储存在硬盘上，硬盘的最小存储单位叫做&amp;rdquo;扇区&amp;rdquo;（Sector）。每个扇区储存512字节（相当于0.5KB）。 操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个&amp;rdquo;块&amp;rdquo;（block）。这种由多个扇区组成的&amp;rdquo;块&amp;rdquo;，是文件存取的最小单位。&amp;rdquo;块&amp;rdquo;的大小，最常见的是4KB，即连续八个 sector组成一个 block。 文件数据都储存在&amp;rdquo;块&amp;rdquo;中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做inode，中文译名为&amp;rdquo;索引节点&amp;rdquo;。
inode记录了什么信息？ inode包含文件的元信息，具体来说有以下内容： + 文件的字节数 + 文件拥有者的User ID + 文件的Group ID + 文件的读、写、执行权限 + 文件的时间戳，共有三个：ctime指inode上一次变动的时间，mtime指文件内容上一次变动的时间，atime指文件上一次打开的时间。 + 链接数，即有多少文件名指向这个inode + 文件数据block的位置
可以用stat命令，查看某个文件的inode信息：stat example.txt 总之，除了文件名以外的所有文件信息，都存在inode之中。
inode的大小 inode也会消耗硬盘空间，所以硬盘格式化的时候，操作系统自动将硬盘分成两个区域。一个是数据区，存放文件数据；另一个是inode区（inode table），存放inode所包含的信息。 每个inode节点的大小，一般是128字节或256字节。inode节点的总数，在格式化时就给定，一般是每1KB或每2KB就设置一个inode。假定在一块1GB的硬盘中，每个inode节点的大小为128字节，每1KB就设置一个inode，那么inode table的大小就会达到128MB，占整块硬盘的12.8%。
查看每个硬盘分区的inode总数和已经使用的数量，可以使用df命令。df -i
查看每个inode节点的大小，可以用如下命令sudo dumpe2fs -h /dev/vdb | grep &amp;quot;inode size&amp;quot;
由于每个文件都必须有一个inode，因此有可能发生inode已经用光，但是硬盘还未存满的情况。这时，就无法在硬盘上创建新文件。
inode号码 每个inode都有一个号码，操作系统用inode号码来识别不同的文件。
这里值得重复一遍，Unix/Linux系统内部不使用文件名，而使用inode号码来识别文件。 对于系统来说，文件名只是inode号码便于识别的别称或者绰号。 表面上，用户通过文件名，打开文件。实际上，系统内部这个过程分成三步：首先，系统找到这个文件名对应的inode号码；其次，通过inode号码，获取inode信息；最后，根据inode信息，找到文件数据所在的block，读出数据。 使用ls -i命令，可以看到文件名对应的inode号码：ls -i example.txt 目录文件 Unix/Linux系统中，目录（directory）也是一种文件。打开目录，实际上就是打开目录文件。
目录文件的结构非常简单，就是一系列目录项（dirent）的列表。每个目录项，由两部分组成：所包含文件的文件名，以及该文件名对应的inode号码。
ls命令只列出目录文件中的所有文件名：ls /etc ls -i命令列出整个目录文件，即文件名和inode号码：ls -i /etc 如果要查看文件的详细信息，就必须根据inode号码，访问inode节点，读取信息。ls -l命令列出文件的详细信息：ls -l /etc
硬链接 一般情况下，文件名和inode号码是&amp;rdquo;一一对应&amp;rdquo;关系，每个inode号码对应一个文件名。但是，Unix/Linux系统允许，多个文件名指向同一个inode号码。这意味着，可以用不同的文件名访问同样的内容；对文件内容进行修改，会影响到所有文件名；但是，删除一个文件名，不影响另一个文件名的访问。这种情况就被称为&amp;rdquo;硬链接&amp;rdquo;（hard link）。 ln命令可以创建硬链接：ln 源文件 目标文件</description>
    </item>
    
    <item>
      <title>Golang学习（一）</title>
      <link>https://toddlerya.github.io/post/golang%E5%AD%A6%E4%B9%A0%E4%B8%80/</link>
      <pubDate>Sun, 24 Dec 2017 12:11:49 +0000</pubDate>
      
      <guid>https://toddlerya.github.io/post/golang%E5%AD%A6%E4%B9%A0%E4%B8%80/</guid>
      <description>闲扯 去北京参加Top100学习到现在一个多月过去了，时间过得好快，转眼2017年只剩下最后一周了，不由得感叹时间就像指缝的流沙。 再加上最近中年程序员不堪重负的各种新闻，感觉自己距离那一天也没多远了，在此之前努力提高自己的姿势水平吧，要没时间了。
在Top100听到了很多讲师提到了Golang，结合之前看过的左耳朵耗子的文章《GO语言、DOCKER 和新技术》，决定要学习了解下Golang了。 最近半个月断断续续看了Golang的一些教程《GO 语言简介（上）— 语法》、《GO 语言简介（下）— 特性》、无闻老师的《Go 编程基础》，只是对Golang有了一点点初步的了解。 学习一门语言最重要的还是要撸代码啊，要动手写个小工具试试手。 恰巧昨天加班时帮同事写了个小工具，感觉用Python的性能不够好，而且想要高性能还要依赖gevent这种第三方库，不方便部署，于是想到用Golang可以来试下，写完编译下给同事用就好啦！昨晚回到家吃过饭陪女朋友玩了会，从十点开始撸代码到凌晨一点，终于写出一个小demo，特此记录下年轻人的第一个Golang小程序。
正文 工具需求是有一个样例zip包，zip包里面有一些bcp数据、xml数据、bjson数据、图片、视频什么的。 zip包的命名规范如下： AAA-BBB-1514090969-CCC_DDD_21234.zip 我们要将zip包的1514090969【绝对秒数】与21234【随机序列】进行替换，随机生成大量的zip包副本，发送到某个ETL输入目录，暂且不管zip包里面的内容（其实zip包内的文件也要随机生成）。 之前同事用Shell写的，没看他代码怎么实现的，不过肯定有问题，一小时才生成了三万个样例zip包，远远达不到压测的要求。 于是我花了十几分钟调用Python的gevent模块帮他重新用Python写了一遍，性能瞬间爆炸。 但是部署gevent比较麻烦（公司和客户都是内网环境，pip是没法用的，只能手动安装所需的第三方包，而且公司大部分操作系统还是Redhat AS6U3，默认的Python是2.6版本，各种不方便），就想到了如果用Golang写一遍是否性能很好，编译后提供二进制文件直接运行就可以了，于是就有了下面的代码。
代码 /* * User: toddlerya * Date: 2017/12/23 * ds接入模块加压工具 */ package main import ( &amp;quot;flag&amp;quot; &amp;quot;fmt&amp;quot; &amp;quot;io&amp;quot; &amp;quot;math/rand&amp;quot; &amp;quot;os&amp;quot; &amp;quot;strconv&amp;quot; &amp;quot;strings&amp;quot; &amp;quot;time&amp;quot; ) func judgeExists(name string) bool { if _, err := os.Stat(name); err != nil { if os.IsNotExist(err) { return false } } return true } func copyFile(src, des string) (w int64, err error) { srcFile, err := os.</description>
    </item>
    
    <item>
      <title>50万Coding用户关系爬爬爬爬</title>
      <link>https://toddlerya.github.io/post/50%E4%B8%87coding%E7%94%A8%E6%88%B7%E5%85%B3%E7%B3%BB%E7%88%AC%E7%88%AC%E7%88%AC%E7%88%AC/</link>
      <pubDate>Sun, 29 Oct 2017 13:05:54 +0000</pubDate>
      
      <guid>https://toddlerya.github.io/post/50%E4%B8%87coding%E7%94%A8%E6%88%B7%E5%85%B3%E7%B3%BB%E7%88%AC%E7%88%AC%E7%88%AC%E7%88%AC/</guid>
      <description>今天我们继续来搞Coding数据~
从官方给出的数据我们知道目前Coding大概有50万注册用户
然后我们还能看到冒泡广场有很多热门用户
那么这些用户之间的关系是什么样子滴？
哪些用户是高冷吸粉狂人？
哪些用户是社交达人？
哪些用户是万年潜水独行侠？
50万Coding小伙伴的社交网络核心是哪位？
是否真的有50万用户？
带着这些疑问，我们开始撸代码！
先把项目地址贴下：https://github.com/toddlerya/AnalyzeCoding
爬虫算法分析设计： 如何获取全部用户信息 通过分析Coding网站, 我们发现有3个地方可以获取到用户数据信息：  冒泡广场: 每一条冒泡都有发布人，有些还有点赞人、打赏人、评论人
 热门用户：冒泡广场首页右侧有20个热门用户，这些用户是当前最活跃的用户，这些用户拥有比较多的粉丝和朋友。
 个人主页：用户的个人主页有一个标签页：关注，这里可以看到此用户关注了哪些人，被哪些人关注。
  现在就是开脑洞的时候啦！ 用户之间的关系分为三种：
 A关注B: A--&amp;gt;B A被B关注：A&amp;lt;--B A与B互相关注：A&amp;lt;--&amp;gt;B  这就是个有向图嘛！
六度分隔理论:
1967年，哈佛大学的心理学教授Stanley Milgram(1933-1984)想要描绘一个连结人与社区的人际连系网。做过一次连锁信实验，结果发现了“六度分隔”现象。简单地说：“你和任何一个陌生人之间所间隔的人不会超过六个，也就是说，最多通过六个人你就能够认识任何一个陌生人。”
思路渐渐清晰了，我们可以采用深度优先算法（Depth-First Search，简称DFS）
从A出发，找到A所有的好友（A1,A2,A3）记录为{A: [A1, A2, A3]}
从A1出发，找到A1所有的好友（A,A2-1,A3-1）记录为{A1: [A, A2-1, A3-1]}
从A2-1出发，找到A2-1所有的好友（A2-1-1）记录为{A2-1: [A2-1-1]}
从A3-1出发，找到A3-1所有的好友（A3-1-1）记录为{A3-1: [A3-1-1]}
从A2出发，找到A2所有的好友（A2-1）记录为{A2: [A2-1]}
……循环递归……
直到Z，Z同学没有任何好友，结束本次遍历爬取。
爬取过程中要注意，已经爬取过的人要跳过，否则会陷入无限循环中。
相关API  当前热门用户：https://coding.net/api/tweet/best_user 用户的朋友们：https://coding.net/api/user/friends/{用户全局唯一代号}?page=1&amp;amp;pageSize=20 用户的粉丝们：https://coding.net/api/user/followers/{用户全局唯一代号}?page=1&amp;amp;pageSize=20 用户个人信息详情：https://coding.net/api/user/key/{用户全局唯一代号}  代码设计实现 数据库设计 为了程序轻便，数据便于分享，我们决定使用Sqlite数据库。
上述分析过程中发送，用户的登录帐号是全局唯一的，不会重复的，我们以此字段作为主键且不允许为空， 还可以通过上述API获取用户的朋友、朋友的个数、用户的粉丝、粉丝的个数、用户详细信息，因此用户社交关系表设计如下：
CREATE TABLE IF NOT EXISTS coding_all_user ( global_key VARCHAR PRIMARY KEY NOT NULL, friends_count INTEGER, friends VARCHAR, followers_count INTEGER, followers VARCHAR )  用户个人信息详情表设计如下：</description>
    </item>
    
    <item>
      <title>利用Python解读十九大工作报告</title>
      <link>https://toddlerya.github.io/post/%E5%88%A9%E7%94%A8python%E8%A7%A3%E8%AF%BB%E5%8D%81%E4%B9%9D%E5%A4%A7%E5%B7%A5%E4%BD%9C%E6%8A%A5%E5%91%8A/</link>
      <pubDate>Sun, 22 Oct 2017 16:23:44 +0000</pubDate>
      
      <guid>https://toddlerya.github.io/post/%E5%88%A9%E7%94%A8python%E8%A7%A3%E8%AF%BB%E5%8D%81%E4%B9%9D%E5%A4%A7%E5%B7%A5%E4%BD%9C%E6%8A%A5%E5%91%8A/</guid>
      <description>关键词: Python、wordcloud、jieba、matplotlib、词云、分词
 前几天的召开的十九大，习近平讲了三小时的三万字工作报告究竟讲了些什么内容呢，我们用Python来一次数据分析看看究竟讲了哪些内容。
主要思路： + 通过jieba分词对工作报告进行切词，清洗，词频统计。 + 通过wordcloud对切词统计结果进行可视化展示。
jieba分词利器 特点  支持三种分词模式：  精确模式，试图将句子最精确地切开，适合文本分析； 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义； 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。
   jieba项目地址：https://github.com/fxsjy/jieba
遇到的问题以及解决办法： 1. 无法匹配最新的词汇 我们采用精确模式进行分词，但是遇到一些词汇在jieba的默认词库没有，所以要根据十九大进行一些定制词库，加载到jieba词库:
import jieba cpc_dict_path = u&#39;user_dict/cpc_dictionary.txt&#39; jieba.load_userdict(cpc_dict_path) # 加载针对全国人民代表大会的分词词典  2. 匹配到了各种符号、空格 切词后统计词频发现有很多标点符号、空格，这些内容我们可以使用正则匹配法进行过滤，u&#39;[\u4e00-\u9fa5]+&#39;匹配所有中文字符，舍弃未命中内容:
import re goal_word = &#39;&#39;.join(re.findall(u&#39;[\u4e00-\u9fa5]+&#39;, seg)).strip() # 过滤所有非中文字符内容  3. 匹配到了很多停词 切词后统计词频发现有很多停词，例如：“的”、“和”、“而且”…… 这种问题肯定不止我遇到了，所以直接去找前人整理好的停词词库即可，通过匹配停词来进行过滤：
stop_words_path = u&#39;user_dict/stopword.txt&#39; with open(stop_words_path) as sf: st_content = sf.readlines() stop_words = [line.strip().decode(&#39;utf-8&#39;) for line in st_content] # 将读取的数据都转为unicode处理 if len(goal_word) !</description>
    </item>
    
    <item>
      <title>Pythoner的vim</title>
      <link>https://toddlerya.github.io/post/pythoner%E7%9A%84vim/</link>
      <pubDate>Fri, 08 Sep 2017 23:08:23 +0000</pubDate>
      
      <guid>https://toddlerya.github.io/post/pythoner%E7%9A%84vim/</guid>
      <description>去年申请的免费aws上个月底到期了（ss梯子没有了T_T），这个月搞了个半年免费的阿里云VPS。 国内的VPS网速果然好快，决定好好利用起来~
正好最近在学Scrapy，感觉用vim写代码，不能自动补全好难受，在公司的内网环境用vim不好折腾插件也就罢了，自己的服务器还是要搞的顺手点，磨刀不误砍柴工嘛。
为了一劳永逸，决定开个坑，维护自己的vim配置，以后换个环境就能开箱使用啦！
先上个项目地址：https://github.com/toddlerya/awesome-vim/
自动补全主要用了jedi-vim插件，这插件太给力了，自动补全方法，还能提示参数，查看文档。
还有一部分配置参考了k-vim-server，这位同学的vim配置很给力，他还有一个完全版的vim插件配置k-vim，大家可以去看看~
使用效果如下： 部署步骤： 1. 备份你的vimrc配置(如果有的话) cp ~/.vimrc ~/.vimrc_bak  2. 安装Vundle git clone https://github.com/gmarik/Vundle.vim.git ~/.vim/bundle/Vundle.vim  3. 安装jedi-vim pip install jedi-vim  4. 下载并设置vimrc git clone https://github.com/toddlerya/awesome-vim.git &amp;amp;&amp;amp; ln -s awesome-vim/vimrc ~/.vimrc  5. 安装Vundle插件 打开vim，运行命令
:PluginInstall  等显示Done后，退出vim就好啦，时间长短看网速，耐心等待。 此过程会安装这几个插件：
davidhalter/jedi-vim tmhedberg/SimpylFold vim-scripts/indentpython.vim jnurmine/Zenburn Lokaltog/powerline  6. 到此就完成啦，享受生活吧！ 2017年09月8日 于 南京
Email
GitHub</description>
    </item>
    
    <item>
      <title>关于NebulaSolarDash</title>
      <link>https://toddlerya.github.io/post/%E5%85%B3%E4%BA%8Enebulasolardash/</link>
      <pubDate>Wed, 10 May 2017 22:06:10 +0000</pubDate>
      
      <guid>https://toddlerya.github.io/post/%E5%85%B3%E4%BA%8Enebulasolardash/</guid>
      <description>2017年的5月1日，三天假期，闭门造了个轮子 写这个工具的目的是为了解决工作问题。 个人工作生产环境无法连接互联网，也没有自建的yum源等，手头又有很多服务器需要进行监控，使用现有的开源方案安装部署是个问题， 各种依赖组件包需要挨个安装，很麻烦，所以想找一款依赖较少部署简单的分布式服务器资源监控工具，找来找去没找到，索性自己动手写一个。 我的本职工作是测试，所以就用最熟悉的Python来写吧，第一次写web应用，先做出来再边学边优化吧。
工具分为客户端和服务端两部分： 服务端使用了bottle来作为web框架，echarts来渲染生成图表； 客户端使用Python原生类库采集服务器资源，客户端采集数据部分代码参考了pyDash
效果如下 项目链接链接 NebulaSolarDash
2017年05月10日 于 南京
Email
GitHub</description>
    </item>
    
    <item>
      <title>Scrapy爬取Coding冒泡广场</title>
      <link>https://toddlerya.github.io/post/scrapy%E7%88%AC%E5%8F%96coding%E5%86%92%E6%B3%A1%E5%B9%BF%E5%9C%BA/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://toddlerya.github.io/post/scrapy%E7%88%AC%E5%8F%96coding%E5%86%92%E6%B3%A1%E5%B9%BF%E5%9C%BA/</guid>
      <description>项目地址: https://github.com/toddlerya/learn_scrapy I. Coding的冒泡广场 CODING 是国内专业的一站式云端软件服务平台，Coding.net 为开发者提供了免费的基础服务，包括但不限于 Git 代码托管，项目管理，Pages 服务，代码质量管理。您可以在 Coding.net 一站完成代码及代码质量，项目及项目人员的管理，Coding.net 让开发变得前所未有的敏捷和简单。
其中Coding有一个冒泡的社交功能&amp;ndash; 冒泡广场，比较像微博，会有很多程序员的日常吐槽，分享等，比如这样：
这里面的数据属性非常丰富，每一条冒泡都具有如下属性：
1. 爬虫分析 爬虫的基本原则是能使用API接口，绝不解析html页面，恰巧这个网站的API很好用。
+ 对这个网站的API进行了基本的分析：html https://coding.net/api/tweet/public_tweets?size=20&amp;amp;sort=time&amp;amp;filter=true&amp;amp;last_time=1504876265000
 分析发现只需提交size和filter参数即可：
 filter=true为只获取精华冒泡，false为获取全量，当然是全量啦！ size为最近发表的多少条冒泡信息
  因此最终的get 请求为： https://coding.net/api/tweet/public_tweets?size=20&amp;amp;filter=false
  2. 这里有两个注意事项  网站的rebots.txt设置了规则，禁止爬取Disallow: /api/*，我们只是做个小实验，不进行大规模的爬取，因此需要修改下Scrapy项目的settings.py配置(不守规矩~):
# Obey robots.txt rules ROBOTSTXT_OBEY = False  size参数如果太大会导致HTTP访问超时，Scrapy报错中止，需要在请求发起时修改Request.meta的参数，见官方文档：DOWNLOAD_TIMEOUT
[scrapy.downloadermiddlewares.retry] DEBUG: Retrying &amp;lt;GET https://coding.net/api/tweet/public_tweets?size=100000&amp;amp;filter=false&amp;gt; (failed 1 times): 504 Gateway Time-out   2017年09月11日 于 南京
Email
GitHub</description>
    </item>
    
    <item>
      <title>[404]</title>
      <link>https://toddlerya.github.io/404/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://toddlerya.github.io/404/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>