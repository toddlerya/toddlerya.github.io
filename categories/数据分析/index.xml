<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>数据分析 on 日拱一卒</title>
    <link>https://toddlerya.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</link>
    <description>Recent content in 数据分析 on 日拱一卒</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 22 Oct 2017 16:23:44 +0000</lastBuildDate>
    
	<atom:link href="https://toddlerya.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>利用Python解读十九大工作报告</title>
      <link>https://toddlerya.github.io/post/%E5%88%A9%E7%94%A8python%E8%A7%A3%E8%AF%BB%E5%8D%81%E4%B9%9D%E5%A4%A7%E5%B7%A5%E4%BD%9C%E6%8A%A5%E5%91%8A/</link>
      <pubDate>Sun, 22 Oct 2017 16:23:44 +0000</pubDate>
      
      <guid>https://toddlerya.github.io/post/%E5%88%A9%E7%94%A8python%E8%A7%A3%E8%AF%BB%E5%8D%81%E4%B9%9D%E5%A4%A7%E5%B7%A5%E4%BD%9C%E6%8A%A5%E5%91%8A/</guid>
      <description>关键词: Python、wordcloud、jieba、matplotlib、词云、分词
 前几天的召开的十九大，习近平讲了三小时的三万字工作报告究竟讲了些什么内容呢，我们用Python来一次数据分析看看究竟讲了哪些内容。
主要思路： + 通过jieba分词对工作报告进行切词，清洗，词频统计。 + 通过wordcloud对切词统计结果进行可视化展示。
jieba分词利器 特点  支持三种分词模式：  精确模式，试图将句子最精确地切开，适合文本分析； 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义； 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。
   jieba项目地址：https://github.com/fxsjy/jieba
遇到的问题以及解决办法： 1. 无法匹配最新的词汇 我们采用精确模式进行分词，但是遇到一些词汇在jieba的默认词库没有，所以要根据十九大进行一些定制词库，加载到jieba词库:
import jieba cpc_dict_path = u&#39;user_dict/cpc_dictionary.txt&#39; jieba.load_userdict(cpc_dict_path) # 加载针对全国人民代表大会的分词词典  2. 匹配到了各种符号、空格 切词后统计词频发现有很多标点符号、空格，这些内容我们可以使用正则匹配法进行过滤，u&#39;[\u4e00-\u9fa5]+&#39;匹配所有中文字符，舍弃未命中内容:
import re goal_word = &#39;&#39;.join(re.findall(u&#39;[\u4e00-\u9fa5]+&#39;, seg)).strip() # 过滤所有非中文字符内容  3. 匹配到了很多停词 切词后统计词频发现有很多停词，例如：“的”、“和”、“而且”…… 这种问题肯定不止我遇到了，所以直接去找前人整理好的停词词库即可，通过匹配停词来进行过滤：
stop_words_path = u&#39;user_dict/stopword.txt&#39; with open(stop_words_path) as sf: st_content = sf.readlines() stop_words = [line.strip().decode(&#39;utf-8&#39;) for line in st_content] # 将读取的数据都转为unicode处理 if len(goal_word) !</description>
    </item>
    
  </channel>
</rss>